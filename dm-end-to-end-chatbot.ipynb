{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import sys\n",
    "# sys.path.append('../..')\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "# openai.api_key  = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"docs/dm.pdf\")\n",
    "pages = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = pages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Typology of Decision-Making Tasks for Visualization\n",
      "Camelia D. Brumar, Sam Molnar, Gabriel Appleby, Kristi Potter, and Remco Chang\n",
      "CHOOSE\n",
      "f1f2f1f2f1f2\n",
      "f1f2f1f2update\n",
      "comparison\n",
      "criteriaoptions\n",
      "f1f2\n",
      "f1f2f3criteriaCREATE\n",
      "f1f2f1f2f1f2\n",
      "f1f2f3f1f2f3f1f2f3f1f2f1f2f1f2\n",
      "f1f2f1f2update\n",
      "scoring\n",
      "criteria\n",
      "Assesses\n",
      "options and\n",
      "outputs a subset\n",
      "deemed optimal\n",
      "or best. ACTIVATE\n",
      "Evaluates\n",
      "the inputs,\n",
      "and only those\n",
      "that meet or exceed\n",
      "a threshold are returned.Represents\n",
      "decisions on\n",
      "assembling,\n",
      "synthesizing,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(page.page_content[0:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'docs/dm.pdf', 'page': 0}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page.metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=300,\n",
    "    length_function=len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='tions with a direct decision-making focus. We added to that list the\\npapers with “decision” mentioned in titles or abstracts to be consistent\\nwith [48], adding up to 74 papers.To refine our selection, we prioritized papers featuring specific do-\\nmain applications, and eliminated generic systems lacking evaluations\\nwith domain experts. This guaranteed that the chosen tools were uti-\\nlized by domain users with actual decisions and requirements that\\ncould be validated. For instance, papers solely based on hypothetical\\ncase studies were excluded (e.g., Podium [63], Zooids [42], etc.). This\\nfiltering process resulted in a final corpus of 69 papers that directly\\ncontribute to our study’s objectives.\\n3.2 Coding Process\\nOur coding process followed an inductive approach to derive the codes,\\ni.e. the design goals for our typology, from the survey corpus outlined\\nin the preceding section. The process involved a team-based approach\\nwith two coders, who are the first two authors of this paper. For each\\npaper, we identified the primary decision-making goal by examining\\nthe abstract and introduction. We identified a few potential goals\\nand then determined the true goal by analyzing the task abstraction,\\nrequirements, evaluation sections, and other supplementary materials,\\nsuch as figures, video demonstrations, and supplementary materials (if\\navailable). In addition, we generated codes for unique decision contexts,\\nsub-decisions that supported the overall decision, and the information\\nthat those decisions would use and produce. The resulting codes are\\ndiscussed in the subsequent subsection.\\n3.3 Design Goals for a Decision Task Typology\\nThe generated codes were used to derive a set of design goals. Through\\nmultiple brainstorming sessions involving all authors, we distilled the\\nfollowing list of goals that we believe our typology should fulfill in\\norder to accurately represent the decision processes described in the', metadata={'source': 'docs/dm.pdf', 'page': 2})"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = TokenTextSplitter(chunk_size=100, chunk_overlap=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "260"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='A Typology of Decision-Making Tasks for Visualization\\nCamelia D. Brumar, Sam Molnar, Gabriel Appleby, Kristi Potter, and Remco Chang\\nCHOOSE\\nf1f2f1f2f1f2\\nf1f2f1f2update\\ncomparison\\ncriteriaoptions\\nf1f2\\nf1f2f3criteriaCREATE\\nf1f2f1f2f1f', metadata={'source': 'docs/dm.pdf', 'page': 0})"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings\n",
    "Let's take our splits and embed them. Using SBERT.net's sentence transformer instead of using the one that the course uses because I don't have an openai api key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cameliadanielabrumar/anaconda3/envs/llama-env/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "# embedding = OpenAIEmbeddings()\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 1. Load a pretrained Sentence Transformer model\n",
    "embedding = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# The sentences to encode\n",
    "# sentences = [\n",
    "#     \"The weather is lovely today.\",\n",
    "#     \"It's so sunny outside!\",\n",
    "#     \"He drove to the stadium.\",\n",
    "# ]\n",
    "\n",
    "# sentence = \"I'm learning how to use Sentence Transformers.\"\n",
    "\n",
    "# 2. Calculate embeddings by calling model.encode()\n",
    "# embeddings = embedding_model.encode(sentences)\n",
    "# embedding = embedding_model.encode(sentence)\n",
    "# print(embedding.shape)\n",
    "# print(type(embedding))\n",
    "# print(embedding.tolist())\n",
    "\n",
    "\n",
    "\n",
    "# [3, 384]\n",
    "\n",
    "# 3. Calculate the embedding similarities\n",
    "# similarities = embedding_model.similarity(embeddings, embeddings)\n",
    "# print(similarities)\n",
    "# tensor([[1.0000, 0.6660, 0.1046],\n",
    "#         [0.6660, 1.0000, 0.1411],\n",
    "#         [0.1046, 0.1411, 1.0000]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually the SBERT.net one didn't work with the langchain vector store. I found some free ones available here: https://python.langchain.com/v0.1/docs/integrations/text_embedding/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example from here: https://python.langchain.com/v0.1/docs/integrations/text_embedding/bge_huggingface/\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "model_name = \"BAAI/bge-small-en\"\n",
    "model_kwargs = {\"device\": \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "hf = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorstores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "persist_directory = 'docs/chroma/'\n",
    "!rm -rf docs/chroma  # remove old database files if any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma.from_documents( # had an error previuously, downgraded to chromadb version 0.4.3 using command: pip install chromadb==0.4.3. See https://github.com/zylon-ai/private-gpt/issues/1012\n",
    "    documents=docs,\n",
    "    embedding=hf,\n",
    "    persist_directory=persist_directory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "260\n"
     ]
    }
   ],
   "source": [
    "print(vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what are the three types of decisions in the typology of decision-making tasks?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_ss = vectordb.similarity_search(question,k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs_ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' real-world visualization\\nsystems.\\n4.1 Decision-Making Tasks\\nOur typology consists of three tasks derived from the scientific\\nliterature [27, 28] : CHOOSE, ACTIV ATE, and CREATE. Each task is\\na function that represents a specific and distinct decision problem. The\\ntype of the inputs to these functions does not change the core process\\nof the decision task. Some of the key differences between the tasks are\\nthe unique transformations of'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_ss[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval\n",
    "We don't want to retrieve only the splits that are the most relevant to the question. We would also like to retrieve a diverse set of splits. We can use Maximum marginal relevance (MMR) for that.\n",
    "\n",
    "Another retrieval method is LLM Aided Retrieval, it takes into consideration the semantic part of the question, and also the metadata part of the question. An LLM can be used to identify the metadata part and use it as a filter for our vectorstore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Addressing Diversity: Maximum marginal relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_mmr = vectordb.max_marginal_relevance_search(question,k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "', the result of our validation can be divided into three broad\\ncategories: the correctness of the typology, the utility of the typol-\\nogy, and how the typology compared to existing task taxonomies and\\ntypologies. The correctness of the typology can be further broken\\ndown into the consideration of completeness and expressiveness. In\\nour interview study, participants found that the typology is complete\\nregarding the three decision tasks and that they could successfully\\nexpress'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_mmr[2].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Addressing Specificity: working with metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GONNA SKIP THIS FOR NOW SINCE THERE IS NO NEED FOR IT IN MY SIMPLE EXAMPLE WITH JUST MY PDF FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional tricks: compression\n",
    "Another approach for improving the quality of retrieved docs is compression.\n",
    "\n",
    "Information most relevant to a query may be buried in a document with a lot of irrelevant text. \n",
    "\n",
    "Passing that full document through your application can lead to more expensive LLM calls and poorer responses.\n",
    "\n",
    "Contextual compression is meant to fix this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_docs(docs):\n",
    "    print(f\"\\n{'-' * 100}\\n\".join([f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap our vectorstore\n",
    "# llm = OpenAI(temperature=0, model=\"gpt-3.5-turbo-instruct\")\n",
    "\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "# To Load Local models through Ollama\n",
    "llm = Ollama(model=\"llama3:8b\", temperature=0.0) # setting temperature to 0.0 to get deterministic results, with low variability and gives us highest fidelity and reliable answers\n",
    "\n",
    "compressor = LLMChainExtractor.from_llm(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=vectordb.as_retriever(search_type=\"mmr\") # or \"similarity\" or \"mmr\" or \"combined\" or \"combined_mmr\" or \"combined_similarity\" <--- this is the search type, recommended by Copilot\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "Extracted relevant parts:\n",
      "\n",
      "...three tasks derived from the scientific literature : CHOOSE, ACTIV ATE, and CREATE...\n",
      "\n",
      "These three types of decisions are relevant to answer the question.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "Extracted relevant part:\n",
      "\n",
      "4 T YPOLOGY OF DECISION -MAKING TASKS\n",
      "As we outlined in the previous section, we created a typology that reflects and captures the ubiquity of decision-making while allowing for flexibility in the execution...\n",
      "\n",
      "This is relevant to answer the question about the three types of decisions in the typology of decision-making tasks.\n"
     ]
    }
   ],
   "source": [
    "compressed_docs = compression_retriever.get_relevant_documents(question)\n",
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA # RetrievalQA is a class that combines a retriever and a compressor to answer questions, it's a retrieval step-backed chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever() # I think this is a similarity based search/retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'According to the provided context, the three types of decisions in the typology of decision-making tasks are:\\n\\n1. CHOOSE\\n2. ACTIVATE\\n3. CREATE\\n\\nThese tasks represent specific and distinct decision problems that can be composed or decomposed into other tasks to capture complex decision-making structures.'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = qa_chain({\"query\": question})\n",
    "result['result']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt that takes in the question and the documents and passes them to the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Build prompt / prompt template\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say \"thanks for asking!\" at the end of the answer. \n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever(),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The three types of decisions in the typology of decision-making tasks are CHOOSE, ACTIVATE, and CREATE. These tasks can be composed or decomposed into other tasks to represent complex decision-making problems. Thanks for asking!'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = qa_chain({\"query\": question})\n",
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=' real-world visualization\\nsystems.\\n4.1 Decision-Making Tasks\\nOur typology consists of three tasks derived from the scientific\\nliterature [27, 28] : CHOOSE, ACTIV ATE, and CREATE. Each task is\\na function that represents a specific and distinct decision problem. The\\ntype of the inputs to these functions does not change the core process\\nof the decision task. Some of the key differences between the tasks are\\nthe unique transformations of', metadata={'page': 3, 'source': 'docs/dm.pdf'})"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"source_documents\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other RetrievalQA chain types\n",
    "### MapReduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain_mr = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever(),\n",
    "    chain_type=\"map_reduce\" # others are refine, and map_rerank\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = qa_chain_mr({\"query\": question}) # takes a while to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result[\"result\"] # the result is not satisfactory, as the llm doesn't find the answer in the paper using this chain type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat (adding memory to keep track of previous questions and answers)\n",
    "## Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True # returns the chat history as a list of messages as opposed to a string, more details on memory in the previous class from this guy\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "retriever=vectordb.as_retriever()\n",
    "qa = ConversationalRetrievalChain.from_llm( # adds something more on top of the retrieval chain, not just memory. It adds a step that takes the history and the new question and condenses it into a standalone question that's passed to the vectorstore to look up relevant documents.               \n",
    "    llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'According to the provided context, the typology of decision-making tasks consists of three tasks:\\n\\n1. CHOOSE\\n2. ACTIVATE\\n3. CREATE\\n\\nThese tasks represent specific and distinct decision problems that can be composed or decomposed into other tasks, allowing for flexibility in the execution of decisions.'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What are the types of decisions in the typology of decision-making tasks?\"\n",
    "result = qa({\"question\": question})\n",
    "result['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"According to the provided context, the key differences between CHOOSE, ACTIVATE, and CREATE types of decisions are not explicitly stated. The text only describes each task briefly without highlighting their unique characteristics or differences.\\n\\nTherefore, I don't know the answer to this question as it is not explicitly mentioned in the given context.\""
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What are the differences between them?\"\n",
    "result = qa({\"question\": question})\n",
    "result['answer']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
