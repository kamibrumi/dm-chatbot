{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook to experiment with the following end to end process: from dataset+task in NL, to typology based diagram, to design recommendation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from pprint import pprint\n",
    "\n",
    "from langchain.document_loaders import NotionDirectoryLoader\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "from langchain.schema.runnable import RunnableMap\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Ollama(model=\"llama3:8b\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine the end goal/decision\n",
    "Input: dataset description\n",
    "\n",
    "Output: The decision task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "documents.extend(PyPDFLoader(\"docs/dm.pdf\").load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "260"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = TokenTextSplitter(chunk_size=100, chunk_overlap=25)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cameliadanielabrumar/anaconda3/envs/llama-env/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/Users/cameliadanielabrumar/anaconda3/envs/llama-env/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "260\n"
     ]
    }
   ],
   "source": [
    "model_name = \"BAAI/bge-small-en\"\n",
    "model_kwargs = {\"device\": \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "hf = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "persist_directory = 'docs/chroma/'\n",
    "!rm -rf docs/chroma  # remove old database files if any\n",
    "\n",
    "vectordb = Chroma.from_documents( # had an error previuously, downgraded to chromadb version 0.4.3 using command: pip install chromadb==0.4.3. See https://github.com/zylon-ai/private-gpt/issues/1012\n",
    "    documents=docs,\n",
    "    embedding=hf,\n",
    "    persist_directory=persist_directory,\n",
    ")\n",
    "retriever = vectordb.as_retriever()\n",
    "\n",
    "print(vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nl_to_typology_goal_template = \"\"\"Imagine you are a visualization designer who wants to understand what are the decisions an expert in embryology and in vitro fertilization would make when designing a visualization.\n",
    "You are tasked with taking the dataset and the task the expert is trying to accomplish and translating the task into one of the decision making tasks that appear in Typology of\n",
    "Decision-Making Tasks for Visualization paper.\n",
    "\n",
    "The three possible decision tasks are: CHOOSE, ACTIVATE and CREATE. Give a brief explanation of the decision making task you chose and why you think it is the most appropriate for the task at hand.\n",
    "When providing reasons, give explanations that relate to the definitions of the three tasks as described in the Typology of Decision-Making Tasks for Visualization paper.\n",
    "The dataset description, task description, and typology of decision making tasks paper are given below. \n",
    "\n",
    "Relevant context from the Typology of Decision-Making Tasks for Visualization paper: {context}\n",
    "Data Description: {data_description}\n",
    "Task Description: {task_description}\n",
    "\"\"\"\n",
    "# prompt = ChatPromptTemplate.from_template(template)\n",
    "nl_to_typology_goal_prompt_template = PromptTemplate.from_template(nl_to_typology_goal_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cameliadanielabrumar/anaconda3/envs/llama-env/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content=' a typology for decision-making tasks in visualiza-\\ntion, addressing the limitations of existing taxonomies. Built upon prior\\nresearch and informed by design goals derived from a thorough liter-\\nature review, the typology comprises three decision tasks: CHOOSE,\\nACTIV ATE, and CREATE. These tasks allow for the representation\\nof complex decision-making structures, as they can be composed or\\ndecomposed into other tasks. The typology demonstrates completeness,', metadata={'page': 8, 'source': 'docs/dm.pdf'}),\n",
       " Document(page_content=' real-world visualization\\nsystems.\\n4.1 Decision-Making Tasks\\nOur typology consists of three tasks derived from the scientific\\nliterature [27, 28] : CHOOSE, ACTIV ATE, and CREATE. Each task is\\na function that represents a specific and distinct decision problem. The\\ntype of the inputs to these functions does not change the core process\\nof the decision task. Some of the key differences between the tasks are\\nthe unique transformations of', metadata={'page': 3, 'source': 'docs/dm.pdf'}),\n",
       " Document(page_content=' created a typology that\\nreflects and captures the ubiquity of decision-making while allowing\\nfor flexibility in the execution of decisions. Here we define the tasks of\\nour typology, focusing on the unique properties of each. We then move\\nto a discussion of the composability of the tasks, followed by four case\\nstudies that illustrate the outlined properties in real-world visualization\\nsystems.\\n4.1 Decision-Making Tasks\\nOur typology consists of three tasks', metadata={'page': 3, 'source': 'docs/dm.pdf'}),\n",
       " Document(page_content=' shows the three decision tasks in our typology: CHOOSE, ACTIVATE, and CREATE. The CHOOSE task assesses\\noptions and outputs a subset deemed optimal or best. The ACTIVATE task represents a decision where options are evaluated, and\\nonly those that meet or exceed a threshold are returned. The CREATE task represents decisions on assembling, synthesizing, or\\ngenerating new information. Together, these three decision tasks can be composed to represent complex decision-making problems.', metadata={'page': 0, 'source': 'docs/dm.pdf'})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm_task_definitions_question = \"What are the three decision making tasks in my typology?\"\n",
    "retriever.get_relevant_documents(dm_task_definitions_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nl_to_typology_goal_chain = RunnableMap({\n",
    "    \"context\": lambda x: retriever.get_relevant_documents(dm_task_definitions_question),\n",
    "    \"data_description\": lambda x: x[\"data\"],\n",
    "    \"task_description\": lambda x: x[\"task\"]\n",
    "}) | nl_to_typology_goal_prompt_template | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After analyzing the dataset and task description, I believe that the most appropriate decision-making task is ACTIVATE.\n",
      "\n",
      "The ACTIVATE task represents a decision where options are evaluated, and only those that meet or exceed a threshold are returned. In this case, the expert in embryology and in vitro fertilization needs to evaluate the medication dose based on various patient parameters (age, BMI, AMH, and AFC) and recommend a dosage for the current patient.\n",
      "\n",
      "The task requires evaluating options (different medication doses) against specific criteria (patient parameters), and only those that meet or exceed a certain threshold (optimal dosage) are returned. This process involves filtering out suboptimal options based on the evaluation of the patient's characteristics, which aligns with the ACTIVATE task definition.\n",
      "\n",
      "In contrast, the CHOOSE task would require selecting one option from a set of available options, which might not accurately capture the complexity of evaluating multiple patient parameters and recommending an optimal dosage. The CREATE task would involve generating new information or assembling existing information to make a decision, which is not applicable in this case since the expert already has the dataset and needs to evaluate it based on specific criteria.\n",
      "\n",
      "Therefore, I believe that the ACTIVATE task best represents the decision-making process required to understand how medication dose varies with patient parameters and recommend an optimal dosage for the current patient.\n"
     ]
    }
   ],
   "source": [
    "nl_to_typology_goal_chain_output =  nl_to_typology_goal_chain.invoke({\"data\": \"tabular data where each row is a patient, and the associated levels of age, bmi amh and afc at the time of the Egg Retrieval Procedure.\",\n",
    "              \"task\": \"understand how the medication dose varies with the following patient parameters: age, bmi amh and afc then recommend a dosage for the current patient.\"\n",
    "              })\n",
    "print(nl_to_typology_goal_chain_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expand the decision tasks iteratively\n",
    "Input: The  decision task.\n",
    "\n",
    "Ouput: a typology based diagram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ACTIVATE'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decision_task = model.invoke(\"What is the decision task the following text is describing? Answer with one of these three words and nothing else: CHOOSE, ACTIVATE or CREATE. \\n\" \n",
    "                             + nl_to_typology_goal_chain_output)\n",
    "\n",
    "decision_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=', during our interview study, we observed a preference among\\nparticipants for a top-down approach (see Section 7). The participantsstarted with high-level decision tasks and recursively decomposed them\\ninto sub-tasks until they achieved the desired level of granularity. In the\\ncase studies below, we demonstrate how these decision-task hierarchies\\ncan be created through a series of decompositions.\\n6 C ASESTUDIES\\nTo illustrate the composability andh', metadata={'page': 4, 'source': 'docs/dm.pdf'}),\n",
       " Document(page_content=' of real-world decision-making\\nproblems, as our tasks can be composed or decomposed into other\\ntasks.arXiv:2404.08812v2  [cs.HC]  22 Apr 2024', metadata={'page': 0, 'source': 'docs/dm.pdf'}),\n",
       " Document(page_content=' This involved dissecting the decision processes, identifying key\\ncomponents, and highlighting other properties relevant to decision-\\nmaking, as outlined in subsection 3.3.\\nFollowing this, we conducted collaborative working sessions with\\nall authors to generate a comprehensive set of decision-making tasks\\nthat met the design goals. This was an iterative process that involved\\ncontinuous refinement and validation of the decision tasks and their\\ndefinitions, drawing upon prominent decision-making theory papers, as\\nwe discuss', metadata={'page': 2, 'source': 'docs/dm.pdf'}),\n",
       " Document(page_content='decision tasks implicitly defines a hierarchical\\nstructure of decision tasks. These hierarchies can be useful as they can\\ndescribe a decision task at a high-level abstraction, while also offering\\ndetailed step-by-step decision-making processes at lower levels. While\\nthese hierarchies can be constructed either from the top-down or the\\nbottom-up, during our interview study, we observed a preference among\\nparticipants for a top-down approach (see Section 7).', metadata={'page': 4, 'source': 'docs/dm.pdf'})]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decomposition_context_question = \"How do you decompose decision making tasks?\"\n",
    "retriever.get_relevant_documents(decomposition_context_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "csv_output_parser = CommaSeparatedListOutputParser()\n",
    "format_instructions = csv_output_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From previous prompt: \n",
    "V2\n",
    "To describe the decomposition, you should enumerate each decision task, describe it, and explain why it is part of the decomposition.\n",
    "Also, you should explain how the information flows from one decision to the next by constructing a node-link diagram.\n",
    "There might be back loops, where one decision task influences a previous decision task.\n",
    "Note that the decision goal is also part of this description, and you should include how the information flows in the goal decision task.\n",
    "The context, decomposition instructions, and decision goal are given below.\n",
    "\n",
    "\n",
    "V1\n",
    "For that, describe the node-link diagram of the decision making process in a couple of tables in CSV format, where each node is a decision task and each edge is a connection between two decision tasks.\n",
    "The first table has the following columns: node id, node type (CHOOSE, ACTIVATE, or CREATE), that is, the name of the decision task, and the description of the decision task. The node id starts at 1, and increments by 1 for each decision task.\n",
    "The second table has the following columns: Source, Target, and the information being passed from one decition to the other. \n",
    "The Source and Target columns should contain the ids of the decision tasks, names of the decision tasks, and the Description column should contain a brief explanation of the connection between the two decision tasks.\n",
    "Add to the nodes and edges tables the decision goal, that is, how do the subtasks relate to the decision goal.\n",
    "Answer with the node and edge tables and nothing else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "typology_goal_to_nl_diagram_template = \"\"\"\n",
    "Imagine you are a visualization designer who wants to understand what are the decisions an expert in embryology and in vitro fertilization would make when recommending medication for ovarian stimulation.\n",
    "You are tasked with taking the decision goal explained as context below, and expanding it according to the decomposition instructions, also given below. \n",
    "The decision goal should be decomposed\n",
    "in {number_of_subtasks} decision subtasks. \n",
    "Each subtask should be also one of the three decision making tasks that appear in Typology of Decision-Making Tasks for Visualization paper (CHOOSE, ACTIVATE, CREATE).\n",
    "\n",
    "For that, describe the node-link diagram of the decision making process in a couple of tables, where each node is a decision task and each edge is a connection between two decision tasks.\n",
    "The first table has the following columns: node id, node type (CHOOSE, ACTIVATE, or CREATE), that is, the name of the decision task, and the description of the decision task. The node id starts at 1, and increments by 1 for each decision task.\n",
    "The second table has the following columns: Source, Target, and the information being passed from one decition to the other. \n",
    "The Source and Target columns should contain the ids of the decision tasks, names of the decision tasks, and the Description column should contain a brief explanation of the connection between the two decision tasks.\n",
    "Add to the nodes and edges tables the decision goal, that is, how do the subtasks relate to the decision goal.\n",
    "Answer with the node and edge tables and nothing else.\n",
    "\n",
    "\n",
    "Relevant context from the Typology of Decision-Making Tasks for Visualization paper: {decision_task_definitions}\n",
    "Relevant decomposition instructions from the Typology of Decision-Making Tasks for Visualization paper: {decomposition_context}\n",
    "Decision Goal Description: {decision_goal_description}\n",
    "\"\"\"\n",
    "# prompt = ChatPromptTemplate.from_template(template)\n",
    "typology_goal_to_nl_diagram_prompt_template = PromptTemplate.from_template(template=typology_goal_to_nl_diagram_template,\n",
    "                                                                        # input_variables=[\"number_of_subtasks\", \"decision_goal_description\"],\n",
    "                                                                        # partial_variables={\"format_instructions\": format_instructions}\n",
    "                                                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "typology_goal_to_nl_diagram_chain = RunnableMap({\n",
    "    \"number_of_subtasks\": lambda x: x[\"number_of_subtasks\"],\n",
    "    \"decision_task_definitions\": lambda x: retriever.get_relevant_documents(dm_task_definitions_question),\n",
    "    \"decomposition_context\": lambda x: retriever.get_relevant_documents(decomposition_context_question),\n",
    "    \"decision_goal_description\": lambda x: x[\"decision_goal_description\"]\n",
    "}) | typology_goal_to_nl_diagram_prompt_template | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the node-link diagram tables:\n",
      "\n",
      "**Nodes Table**\n",
      "\n",
      "| Node ID | Node Type | Description |\n",
      "| --- | --- | --- |\n",
      "| 1 | ACTIVATE | Evaluate options (different medication doses) against specific criteria (patient parameters) and recommend a dosage for the current patient. |\n",
      "| 2 | CHOOSE | Select one option from a set of available options based on patient characteristics. |\n",
      "| 3 | CREATE | Generate new information or assemble existing information to make a decision about medication dose recommendation. |\n",
      "\n",
      "**Edges Table**\n",
      "\n",
      "| Source | Target | Description |\n",
      "| --- | --- | --- |\n",
      "| 1 | 2 | Filter out suboptimal options based on evaluation of patient's characteristics, and only recommend optimal dosage. |\n",
      "| 1 | 3 | Not applicable - expert already has the dataset and needs to evaluate it based on specific criteria. |\n",
      "\n",
      "**Decision Goal**\n",
      "\n",
      "The decision goal is to ACTIVATE a recommendation for medication dose based on patient parameters (age, BMI, AMH, and AFC). The subtasks are:\n",
      "\n",
      "* Node 1: ACTIVATE - Evaluate options against specific criteria and recommend a dosage.\n",
      "* Node 2: CHOOSE - Not applicable in this case, as it would require selecting one option from a set of available options, which is not the decision-making process required here.\n",
      "* Node 3: CREATE - Not applicable in this case, as it would involve generating new information or assembling existing information to make a decision, which is not necessary given the dataset and specific criteria.\n"
     ]
    }
   ],
   "source": [
    "typology_goal_to_nl_diagram_chain_output =  typology_goal_to_nl_diagram_chain.invoke({\"number_of_subtasks\": 3,\n",
    "              \"decision_goal_description\": nl_to_typology_goal_chain_output\n",
    "              })\n",
    "print(typology_goal_to_nl_diagram_chain_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note to self: this can be improved by adding memory to the llm, that is, I shouldn't have to pass it the previous output, to every new prompt template, but I should just use a template.\n",
    "See the memory module in this course: https://learn.deeplearning.ai/courses/langchain/lesson/3/memory\n",
    "\n",
    "From natural language to pandas diagram. See example of how to format the graph in a table here: https://towardsdatascience.com/visualizing-networks-in-python-d70f4cbeb259"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "# csv_output_parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "# nodes_template = \"\"\"\n",
    "# Convert the decision process described below to a table in CSV format with 3 columns: 'decision_id', 'decision_type' and 'decision_description'. \n",
    "# 'decision_id' is the id of the decision, which starts at 1 and increments by 1 for each decision. \n",
    "# 'decision_type' is one of the three decision types: CHOOSE, ACTIVATE, CREATE. \n",
    "# 'decision_description' is the description of the decision. \n",
    "# Don't forget to add the names of the columns.\n",
    "# Please don't add an introductory sentence introducing the table in CSV format.\n",
    "\n",
    "# Here is the context for the decision process: {decision_process}\n",
    "\n",
    "# \"\"\"\n",
    "# format_instructions = csv_output_parser.get_format_instructions()\n",
    "# nodes_prompt = PromptTemplate(\n",
    "#     template=nodes_template,\n",
    "#     input_variables=[\"decision_process\"],\n",
    "#     # partial_variables={\"format_instructions\": format_instructions},\n",
    "# )\n",
    "\n",
    "# nodes_chain = nodes_prompt | model | csv_output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nodes_chain.invoke({\"decision_process\": typology_goal_to_nl_diagram_chain_output})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edges_prompt = PromptTemplate(\n",
    "#     template=\"Convert the information flow in the decision process described below to a table in CSV format with 3 columns: 'source', 'target', and 'description'. The 'source' is the decision task that generates the information, 'target' is the decision task that receives that information, and 'description' is the description on how the information flows from one decision task to the next. Don't forget to add the names of the columns. \\n{decision_process}\\n{format_instructions}\\n Please don't add an introductory sentence introducing the table in CSV format.\",\n",
    "#     input_variables=[\"decision_process\"],\n",
    "#     partial_variables={\"format_instructions\": format_instructions},\n",
    "# )\n",
    "\n",
    "# edges_chain = edges_prompt | model | csv_output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edges_csv = edges_chain.invoke({\"decision_process\": typology_goal_to_nl_diagram_chain_output})\n",
    "# edges_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the json parser\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing import List\n",
    "\n",
    "# Define your desired data structure.\n",
    "# took the structure from here: https://gist.github.com/mbostock/4062045\n",
    "class Node(BaseModel):\n",
    "    node_id: int = Field(description=\"this field is called 'id', and it is the id of the node in the graph, starting at 1.\")\n",
    "    node_type: dict = Field(description=\"this field is called 'node_type'. The type of the node is one of the decision making tasks: CHOOSE, ACTIVATE or CREATE\")\n",
    "    node_description: dict = Field(description=\"this field is called 'description'. This is the description of the node, that is, the description of the decision making task\")\n",
    "\n",
    "class Edge(BaseModel):\n",
    "    source: dict = Field(description=\"this field is called 'source', and it is the id of the source node in the graph\")\n",
    "    target: dict = Field(description=\"this field is called 'target', and it is the id of the target node in the graph\")\n",
    "    description: dict = Field(description=\"This is the description of the edge, that is, the description of the information flow from one node to the next\")\n",
    "\n",
    "class Graph(BaseModel):\n",
    "    nodes: List[Node] = Field(description=\"list of nodes in the graph\")\n",
    "    edges: List[Edge] = Field(description=\"list of edges in the graph\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And a query intented to prompt a language model to populate the data structure.\n",
    "query = '''From the decision process described below, extract a node-link diagram where the nodes are decision nodes (CHOOSE, ACTIVATE or CREATE), \n",
    "and the edges represent the information flow between the decision nodes.\n",
    "Make sure that the nodes have an id and that the edges use the node ids. \n",
    "The decision process is described below in two parts: as a table in CSV format of nodes and as a table in CSV format of edges: \\n ''' + typology_goal_to_diagram_chain_output\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = JsonOutputParser(pydantic_object=Graph)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "output_json = chain.invoke({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nodes': [{'node_id': 1,\n",
       "   'node_type': {'type': 'CHOOSE'},\n",
       "   'node_description': {'description': 'Identify relevant patient parameters (age, BMI, AMH, and AFC) that affect medication dose.'}},\n",
       "  {'node_id': 2,\n",
       "   'node_type': {'type': 'ACTIVATE'},\n",
       "   'node_description': {'description': 'Evaluate different medication doses based on the chosen patient parameters and recommend a dosage for the current patient.'}}],\n",
       " 'edges': [{'source': 1,\n",
       "   'target': 2,\n",
       "   'description': {'description': 'Information flows from this decision to the next step, as the chosen patient parameters will be used to evaluate different medication doses.'}}]}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'source': 1,\n",
       "  'target': 2,\n",
       "  'description': 'Information flows from this decision to the next step, as the chosen patient parameters will be used to evaluate different medication doses.'}]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_json['edges']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xq/gb72q1px7yb_s35rx4h5l0n00000gn/T/ipykernel_28396/3712040430.py:3: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  nodes_df = pd.read_json(json.dumps(output_json['properties']['nodes']['items']))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Mixing dicts with non-Series may lead to ambiguous ordering.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[121], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m nodes_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_json\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mproperties\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnodes\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mitems\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m nodes_df\n",
      "File \u001b[0;32m~/anaconda3/envs/llama-env/lib/python3.12/site-packages/pandas/io/json/_json.py:815\u001b[0m, in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options, dtype_backend, engine)\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m json_reader\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 815\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/llama-env/lib/python3.12/site-packages/pandas/io/json/_json.py:1025\u001b[0m, in \u001b[0;36mJsonReader.read\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_object_parser(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_lines(data_lines))\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1025\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_object_parser\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype_backend \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[1;32m   1027\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mconvert_dtypes(\n\u001b[1;32m   1028\u001b[0m         infer_objects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype_backend\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype_backend\n\u001b[1;32m   1029\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/llama-env/lib/python3.12/site-packages/pandas/io/json/_json.py:1051\u001b[0m, in \u001b[0;36mJsonReader._get_object_parser\u001b[0;34m(self, json)\u001b[0m\n\u001b[1;32m   1049\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1050\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframe\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1051\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[43mFrameParser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1053\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseries\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1054\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, \u001b[38;5;28mbool\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/llama-env/lib/python3.12/site-packages/pandas/io/json/_json.py:1187\u001b[0m, in \u001b[0;36mParser.parse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1185\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 1187\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1189\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/llama-env/lib/python3.12/site-packages/pandas/io/json/_json.py:1402\u001b[0m, in \u001b[0;36mFrameParser._parse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1399\u001b[0m orient \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morient\n\u001b[1;32m   1401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m orient \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1402\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;241m=\u001b[39m \u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mujson_loads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprecise_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecise_float\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m   1404\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1405\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m orient \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1406\u001b[0m     decoded \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   1407\u001b[0m         \u001b[38;5;28mstr\u001b[39m(k): v\n\u001b[1;32m   1408\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m ujson_loads(json, precise_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecise_float)\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   1409\u001b[0m     }\n",
      "File \u001b[0;32m~/anaconda3/envs/llama-env/lib/python3.12/site-packages/pandas/core/frame.py:778\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    772\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[1;32m    773\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[1;32m    774\u001b[0m     )\n\u001b[1;32m    776\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    777\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 778\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[0;32m~/anaconda3/envs/llama-env/lib/python3.12/site-packages/pandas/core/internals/construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[1;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[0;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/llama-env/lib/python3.12/site-packages/pandas/core/internals/construction.py:114\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[0;32m~/anaconda3/envs/llama-env/lib/python3.12/site-packages/pandas/core/internals/construction.py:680\u001b[0m, in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    677\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll arrays must be of the same length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n\u001b[0;32m--> 680\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    681\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    682\u001b[0m     )\n\u001b[1;32m    684\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_series:\n\u001b[1;32m    685\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m lengths[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(index):\n",
      "\u001b[0;31mValueError\u001b[0m: Mixing dicts with non-Series may lead to ambiguous ordering."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "nodes_df = pd.read_json(json.dumps(output_json['properties']['nodes']['items']))\n",
    "nodes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xq/gb72q1px7yb_s35rx4h5l0n00000gn/T/ipykernel_28396/1606649814.py:1: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  edges_df = pd.read_json(json.dumps(output_json['properties']['links']['items']))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>The CHOOSE task provides the relevant patient ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   source  target                                        description\n",
       "0       1       2  The CHOOSE task provides the relevant patient ..."
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges_df = pd.read_json(json.dumps(output_json['properties']['links']['items']))\n",
    "edges_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.from_pandas_edgelist(edges_df, 'source', 'target', 'description')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_attributes = nodes_df.set_index('id')['type'].to_dict()\n",
    "nx.set_node_attributes(G, node_attributes, 'node_type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AtlasView({2: {'description': 'The CHOOSE task provides the relevant patient parameters, which are used to evaluate medication options in the ACTIVATE task.'}})"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
