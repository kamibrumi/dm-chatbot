{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.chains import RetrievalQA,  ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader = PyPDFLoader(\"docs/*.pdf\")\n",
    "# pages = loader.load()\n",
    "\n",
    "loaders = [\n",
    "    PyPDFLoader(path) for path in glob.glob(\"docs/*.pdf\")\n",
    "]\n",
    "pages = []\n",
    "for loader in loaders:\n",
    "    pages.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "344"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pages) # there are more docs than loaders because each pdf has multiple pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(loaders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = TokenTextSplitter(chunk_size=100, chunk_overlap=25)\n",
    "docs = text_splitter.split_documents(pages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the embeddings and the vectorstore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cameliadanielabrumar/anaconda3/envs/llama-env/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/Users/cameliadanielabrumar/anaconda3/envs/llama-env/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8834\n"
     ]
    }
   ],
   "source": [
    "model_name = \"BAAI/bge-small-en\"\n",
    "model_kwargs = {\"device\": \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "hf = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "persist_directory = 'docs/chroma/'\n",
    "!rm -rf docs/chroma  # remove old database files if any\n",
    "\n",
    "vectordb = Chroma.from_documents( # had an error previuously, downgraded to chromadb version 0.4.3 using command: pip install chromadb==0.4.3. See https://github.com/zylon-ai/private-gpt/issues/1012\n",
    "    documents=docs,\n",
    "    embedding=hf,\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "\n",
    "print(vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the local llm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model=\"llama3:8b\", temperature=0.0) # setting temperature to 0.0 to get deterministic results, with low variability and gives us highest fidelity and reliable answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question answering with prompts and memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[*Deprecated*] Chain for having a conversation based on retrieved documents.\\n\\n    This class is deprecated. See below for an example implementation using\\n    `create_retrieval_chain`. Additional walkthroughs can be found at\\n    https://python.langchain.com/docs/use_cases/question_answering/chat_history\\n\\n        .. code-block:: python\\n\\n            from langchain.chains import (\\n                create_history_aware_retriever,\\n                create_retrieval_chain,\\n            )\\n            from langchain.chains.combine_documents import create_stuff_documents_chain\\n            from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\\n            from langchain_openai import ChatOpenAI\\n\\n\\n            retriever = ...  # Your retriever\\n\\n            llm = ChatOpenAI()\\n\\n            # Contextualize question\\n            contextualize_q_system_prompt = (\\n                \"Given a chat history and the latest user question \"\\n                \"which might reference context in the chat history, \"\\n                \"formulate a standalone question which can be understood \"\\n                \"without the chat history. Do NOT answer the question, just \"\\n                \"reformulate it if needed and otherwise return it as is.\"\\n            )\\n            contextualize_q_prompt = ChatPromptTemplate.from_messages(\\n                [\\n                    (\"system\", contextualize_q_system_prompt),\\n                    MessagesPlaceholder(\"chat_history\"),\\n                    (\"human\", \"{input}\"),\\n                ]\\n            )\\n            history_aware_retriever = create_history_aware_retriever(\\n                llm, retriever, contextualize_q_prompt\\n            )\\n\\n            # Answer question\\n            qa_system_prompt = (\\n                \"You are an assistant for question-answering tasks. Use \"\\n                \"the following pieces of retrieved context to answer the \"\\n                \"question. If you don\\'t know the answer, just say that you \"\\n                \"don\\'t know. Use three sentences maximum and keep the answer \"\\n                \"concise.\"\\n                \"\\n\\n\"\\n                \"{context}\"\\n            )\\n            qa_prompt = ChatPromptTemplate.from_messages(\\n                [\\n                    (\"system\", qa_system_prompt),\\n                    MessagesPlaceholder(\"chat_history\"),\\n                    (\"human\", \"{input}\"),\\n                ]\\n            )\\n            # Below we use create_stuff_documents_chain to feed all retrieved context\\n            # into the LLM. Note that we can also use StuffDocumentsChain and other\\n            # instances of BaseCombineDocumentsChain.\\n            question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\\n            rag_chain = create_retrieval_chain(\\n                history_aware_retriever, question_answer_chain\\n            )\\n\\n            # Usage:\\n            chat_history = []  # Collect chat history here (a sequence of messages)\\n            rag_chain.invoke({\"input\": query, \"chat_history\": chat_history})\\n\\n    This chain takes in chat history (a list of messages) and new questions,\\n    and then returns an answer to that question.\\n    The algorithm for this chain consists of three parts:\\n\\n    1. Use the chat history and the new question to create a \"standalone question\".\\n    This is done so that this question can be passed into the retrieval step to fetch\\n    relevant documents. If only the new question was passed in, then relevant context\\n    may be lacking. If the whole conversation was passed into retrieval, there may\\n    be unnecessary information there that would distract from retrieval.\\n\\n    2. This new question is passed to the retriever and relevant documents are\\n    returned.\\n\\n    3. The retrieved documents are passed to an LLM along with either the new question\\n    (default behavior) or the original question and chat history to generate a final\\n    response.\\n\\n    Example:\\n        .. code-block:: python\\n\\n            from langchain.chains import (\\n                StuffDocumentsChain, LLMChain, ConversationalRetrievalChain\\n            )\\n            from langchain_core.prompts import PromptTemplate\\n            from langchain_community.llms import OpenAI\\n\\n            combine_docs_chain = StuffDocumentsChain(...)\\n            vectorstore = ...\\n            retriever = vectorstore.as_retriever()\\n\\n            # This controls how the standalone question is generated.\\n            # Should take `chat_history` and `question` as input variables.\\n            template = (\\n                \"Combine the chat history and follow up question into \"\\n                \"a standalone question. Chat History: {chat_history}\"\\n                \"Follow up question: {question}\"\\n            )\\n            prompt = PromptTemplate.from_template(template)\\n            llm = OpenAI()\\n            question_generator_chain = LLMChain(llm=llm, prompt=prompt)\\n            chain = ConversationalRetrievalChain(\\n                combine_docs_chain=combine_docs_chain,\\n                retriever=retriever,\\n                question_generator=question_generator_chain,\\n            )\\n    \\n\\nNotes\\n-----\\n.. deprecated:: 0.1.17\\n   Use create_history_aware_retriever together with create_retrieval_chain (see example in docstring) instead.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ConversationalRetrievalChain.__doc__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True # returns the chat history as a list of messages as opposed to a string, more details on memory in the previous class from this guy\n",
    ")\n",
    "\n",
    "retriever=vectordb.as_retriever(search_type=\"mmr\")\n",
    "qa = ConversationalRetrievalChain.from_llm( # adds something more on top of the retrieval chain, not just memory. It adds a step that takes the history and the new question and condenses it into a standalone question that's passed to the vectorstore to look up relevant documents.               \n",
    "    llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chatting with the llm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'According to the provided context, the typology of decision-making tasks consists of three tasks:\\n\\n1. CHOOSE\\n2. ACTIVATE\\n3. CREATE\\n\\nThese tasks represent specific and distinct decision problems.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What are the types of decisions in the typology of decision-making tasks?\"\n",
    "result = qa({\"question\": question})\n",
    "result['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'According to the provided context, the three types of decisions in the typology of decision-making tasks are:\\n\\n1. CHOOSE\\n2. ACTIVATE\\n3. CREATE\\n\\nThese tasks were derived from scientific literature and represent specific and distinct decision problems.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What are the 3 types again??\"\n",
    "result = qa({\"question\": question})\n",
    "result['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided context, an example of a \"CHOOSE\" task is not explicitly mentioned. However, it can be inferred that a \"CHOOSE\" task assesses options and outputs a subset deemed optimal or best.\\n\\nIn the given examples, the top predictions of both options represent \"Read Email\" but have different probabilities. This could be considered as an example of a decision-making problem where options are evaluated to determine the best choice. However, it is not explicitly labeled as a \"CHOOSE\" task.\\n\\nTherefore, I don\\'t know the specific answer to this question based on the provided context.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Give me an example of a choose task.\"\n",
    "result = qa({\"question\": question})\n",
    "result['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'According to the context, an example of a CHOOSE task is buying a car. In this scenario, we have four cars from which we can choose. We could score the cars on their features relative to the other cars in the set, and select the car with the top score.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Give me a real-world example of that task found in one of the provided documents.\"\n",
    "result = qa({\"question\": question})\n",
    "result['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'VectorStoreRetriever' object has no attribute 'retrieved_docs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mretriever\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieved_docs\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'VectorStoreRetriever' object has no attribute 'retrieved_docs'"
     ]
    }
   ],
   "source": [
    "retriever.retrieved_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
